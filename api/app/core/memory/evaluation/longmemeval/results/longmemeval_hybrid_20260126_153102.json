{
  "dataset": "longmemeval",
  "items": 3,
  "accuracy_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "f1_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "jaccard_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "samples": [
    {
      "question": "What degree did I graduate with?",
      "prediction": "Business Administration",
      "answer": "Business Administration",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "e47becba",
      "options": [],
      "context_count": 8,
      "context_chars": 678,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 7,
      "metrics": {
        "exact_match": true,
        "f1": 1.0,
        "jaccard": 1.0
      },
      "timing": {
        "search_ms": 1478.9669513702393,
        "llm_ms": 3227.4386882781982
      }
    },
    {
      "question": "How long is my daily commute to work?",
      "prediction": "45 minutes each way",
      "answer": "45 minutes each way",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "118b2229",
      "options": [],
      "context_count": 15,
      "context_chars": 1421,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 14,
      "metrics": {
        "exact_match": true,
        "f1": 1.0,
        "jaccard": 1.0
      },
      "timing": {
        "search_ms": 1681.0529232025146,
        "llm_ms": 2154.082775115967
      }
    },
    {
      "question": "Where did I redeem a $5 coupon on coffee creamer?",
      "prediction": "Unknown",
      "answer": "Target",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "51a45a95",
      "options": [],
      "context_count": 9,
      "context_chars": 729,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 8,
      "metrics": {
        "exact_match": false,
        "f1": 0.0,
        "jaccard": 0.0
      },
      "timing": {
        "search_ms": 1191.382646560669,
        "llm_ms": 2328.467845916748
      }
    }
  ],
  "latency": {
    "search": {
      "mean": 1450.4675070444744,
      "p50": 1478.9669513702393,
      "p95": 1660.844326019287,
      "iqr": 244.83513832092285
    },
    "llm": {
      "mean": 2569.996436436971,
      "p50": 2328.467845916748,
      "p95": 3137.541604042053,
      "iqr": 536.6779565811157
    }
  },
  "context": {
    "avg_tokens": 151.66666666666666,
    "avg_chars": 942.6666666666666,
    "count_avg": 10.666666666666666
  },
  "params": {
    "end_user_id": "longmemeval_zh_bak_3",
    "search_limit": 8,
    "context_char_budget": 4000,
    "search_type": "hybrid",
    "llm_id": "2c9b0782-7a85-4740-ba84-4baf77f256c4",
    "embedding_id": "e2a6392d-ca63-4d59-a523-647420b59cb2",
    "sample_size": 3,
    "start_index": 0
  },
  "timestamp": "2026-01-26T15:31:02.723841",
  "metric_summary": {
    "score_accuracy": 66.66666666666666,
    "latency_median_s": 3.8351356983184814,
    "latency_iqr_s": 0.5932775735855103,
    "avg_context_tokens_k": 0.15166666666666664
  },
  "diagnostics": {
    "duplicate_previews_top": [],
    "unique_preview_count": 3
  }
}