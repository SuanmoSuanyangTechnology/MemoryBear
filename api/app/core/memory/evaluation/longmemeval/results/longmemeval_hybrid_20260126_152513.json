{
  "dataset": "longmemeval",
  "items": 3,
  "accuracy_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "f1_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "jaccard_by_type": {
    "single-session-user": 0.6666666666666666
  },
  "samples": [
    {
      "question": "What degree did I graduate with?",
      "prediction": "Business Administration",
      "answer": "Business Administration",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "e47becba",
      "options": [],
      "context_count": 11,
      "context_chars": 1057,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 10,
      "metrics": {
        "exact_match": true,
        "f1": 1.0,
        "jaccard": 1.0
      },
      "timing": {
        "search_ms": 1501.5990734100342,
        "llm_ms": 3345.266819000244
      }
    },
    {
      "question": "How long is my daily commute to work?",
      "prediction": "45 minutes each way",
      "answer": "45 minutes each way",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "118b2229",
      "options": [],
      "context_count": 11,
      "context_chars": 1021,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 10,
      "metrics": {
        "exact_match": true,
        "f1": 1.0,
        "jaccard": 1.0
      },
      "timing": {
        "search_ms": 1178.6537170410156,
        "llm_ms": 2658.588647842407
      }
    },
    {
      "question": "Where did I redeem a $5 coupon on coffee creamer?",
      "prediction": "Unknown",
      "answer": "Target",
      "question_type": "single-session-user",
      "is_temporal": false,
      "question_id": "51a45a95",
      "options": [],
      "context_count": 9,
      "context_chars": 764,
      "retrieved_dialogue_count": 0,
      "retrieved_statement_count": 8,
      "metrics": {
        "exact_match": false,
        "f1": 0.0,
        "jaccard": 0.0
      },
      "timing": {
        "search_ms": 1400.2289772033691,
        "llm_ms": 2110.689401626587
      }
    }
  ],
  "latency": {
    "search": {
      "mean": 1360.1605892181396,
      "p50": 1400.2289772033691,
      "p95": 1491.4620637893677,
      "iqr": 161.47267818450928
    },
    "llm": {
      "mean": 2704.848289489746,
      "p50": 2658.588647842407,
      "p95": 3276.5990018844604,
      "iqr": 617.2887086868286
    }
  },
  "context": {
    "avg_tokens": 152.66666666666666,
    "avg_chars": 947.3333333333334,
    "count_avg": 10.333333333333334
  },
  "params": {
    "end_user_id": "longmemeval_zh_bak_3",
    "search_limit": 8,
    "context_char_budget": 4000,
    "search_type": "hybrid",
    "llm_id": "2c9b0782-7a85-4740-ba84-4baf77f256c4",
    "embedding_id": "e2a6392d-ca63-4d59-a523-647420b59cb2",
    "sample_size": 3,
    "start_index": 0
  },
  "timestamp": "2026-01-26T15:25:12.889747",
  "metric_summary": {
    "score_accuracy": 66.66666666666666,
    "latency_median_s": 3.837242364883423,
    "latency_iqr_s": 0.6679737567901611,
    "avg_context_tokens_k": 0.15266666666666664
  },
  "diagnostics": {
    "duplicate_previews_top": [],
    "unique_preview_count": 3
  }
}